{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics and Distance based Features\n",
    "\n",
    "### Statistic based feature is `groupby` feature.\n",
    "\n",
    "Dataset example:\n",
    "\n",
    "![ctr_task.png](../images/ctr_task.png)\n",
    "\n",
    "\n",
    "Approaches to solve the task:\n",
    "\n",
    "1. The most strightforward approach is apply `LabelEncoding` on `Ad_position` feature. In this case, model captures all hiden relationships between variables, but no matter how good it is it still treats all the data points independently.  \n",
    "\n",
    "\n",
    "2. Perform feature engineering to derive intersting relationships of `user-page` pair.\n",
    "\n",
    "![user_page.png](../images/user_page.png)\n",
    "\n",
    "![user_page_imp.png](../images/user_page_imp.png)\n",
    "\n",
    "\n",
    "\n",
    "On the same way we can create similar usefull features.\n",
    "- How many pages user visited\n",
    "- Standard deviation of prices\n",
    "- Most visited page\n",
    "- Many many more\n",
    "\n",
    "\n",
    "### Distance based feature is `kNN` feature.\n",
    "\n",
    "- Explicit group is not needed\n",
    "- More flexible\n",
    "- Much harder to implement\n",
    "\n",
    "On house price prediction competition, distance based features can be usefull. For example:\n",
    "\n",
    "1. Number of houses in 500m, 1000m,..\n",
    "2. Average price per square meter in 500m, 1000m,..\n",
    "3. Number of schools/supermarkets/parking lots in 500m, 1000m,..\n",
    "4. Distance to closest subway station\n",
    "\n",
    "\n",
    "Not only geographical positions, but can apply to others too. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization for Feature Engineering\n",
    "\n",
    "Not clear how to use it for feature engineering.\n",
    "\n",
    "- Matrix Factorization is a very general approach fordimensionality reduction and feature extraction.\n",
    "- It can be applied for transforming categorical features into real-valued.\n",
    "- Many of tricks trick suitable for linear models can be useful for MF.\n",
    "\n",
    "Reduces dimension using latent facors. Look at sklearn SVD and TruncatedSVD.\n",
    "\n",
    "```python\n",
    "# Right way:\n",
    "X_all= np.concatenate([X_train,X_test])\n",
    "pca.fit(X_all)\n",
    "X_train_pca= pca.transform(X_train)\n",
    "X_test_pca= pca.transform(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Interactions\n",
    "\n",
    "Very usefull for tree-based models. \n",
    "\n",
    "![banner_selection_example.png](../images/banner_selection_example.png)\n",
    "\n",
    "\n",
    "`ad_site` is combination of `category_ad` and `category_site`. \n",
    "\n",
    "Why we need it? Because model treat each feature independently, by combining them we add some knowledge to model. \n",
    "\n",
    "\n",
    "Implementation:\n",
    "\n",
    "1) First variant:\n",
    "\n",
    "![interaction_imp1.png](../images/interaction_imp1.png)\n",
    "\n",
    "\n",
    "2) Second variant:\n",
    "\n",
    "![interaction_imp2.png](../images/interaction_imp2.png)\n",
    "\n",
    "\n",
    "Both implementation results are the same. \n",
    "\n",
    "\n",
    "Interaction can be applied for numerical features too.\n",
    "\n",
    "Combine both numerical features by:\n",
    "\n",
    "- Multiplication\n",
    "- Sum\n",
    "- Diff\n",
    "- Division\n",
    "\n",
    "Example:\n",
    "\n",
    "![interaction_numeric.png](../images/interaction_numeric.png)\n",
    "\n",
    "\n",
    "Practical Notes:\n",
    "\n",
    "- We have a lot of possible interactions −N*N for Nfeatures.\n",
    "- Need to reduce its’ number\n",
    "     - a.Dimensionality reduction (Inpractical, don't use)\n",
    "     - b.Feature selection. (Practically usefull, because only several combinations are important. To find important combinations, run RandomForest or XGBoost and select most import feature combination.)\n",
    "     \n",
    "     \n",
    "- 2nd order intearctions. (only 2 features combination),  easy to construct as above example. For higher order, use:\n",
    "\n",
    "```python\n",
    "# in sklearn\n",
    "tree_model.apply()\n",
    "\n",
    "# in xgboost\n",
    "booster.predict(pred_leaf=True)\n",
    "```\n",
    "\n",
    "## t-SNE\n",
    "\n",
    "- tSNE is a great tool for visualization\n",
    "- It can be used as feature as well\n",
    "- Be careful with interpretation of results\n",
    "- Try different perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
