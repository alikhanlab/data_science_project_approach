{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling\n",
    "\n",
    "\n",
    "## Definition\n",
    "\n",
    "Combining different machine learning models to get a better prediction.\n",
    "\n",
    "## Methods\n",
    "\n",
    "- Averaging(or blending)\n",
    "- Weighted averaging\n",
    "- Conditional averaging\n",
    "- Bagging \n",
    "- Boosting\n",
    "- Stacking\n",
    "- StackNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging\n",
    "\n",
    "\n",
    "![image.png](images/ensemble_averaging_1.png)\n",
    "![image.png](images/ensemble_averaging_2.png)\n",
    "![image.png](images/ensemble_averaging_3.png)\n",
    "\n",
    "\n",
    "#### Caution: This example shows how it should theoretically, but there is no way to find `if age < 50` without using real target value. \n",
    "![image.png](images/ensemble_averaging_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "### Definition\n",
    "\n",
    "Bagging means averaging slightly different versions of the same model to imporove accuracy. \n",
    "\n",
    "### Why bagging \n",
    "\n",
    "There are 2 main sources of errors in modeling:\n",
    "1. Errors due to Bias (underfitting)\n",
    "2. Error due to Variance (overfitting)\n",
    "\n",
    "Bagging tries to find average of underfitting and overfitting.\n",
    "\n",
    "Bagging model of decision trees is Random Forest.\n",
    "\n",
    "### Parameters that control bagging\n",
    "\n",
    "- Changing the seed\n",
    "- Row(Sub) sampling or Bootstrapping\n",
    "- Shuffling (for some models order of rows matter)\n",
    "- Column(Sub) sampling\n",
    "- Model-specific parameters (like regularization parameter for linear models)\n",
    "- Number of models (or bags)\n",
    "- (Optionally) parallelism. Bags are independent of each other.\n",
    "\n",
    "![image.png](images/example_bagging.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "### Definition\n",
    "\n",
    "A form of weighted averaging of models where each model is built sequentially via taking into account the past model performance. \n",
    "\n",
    "## Main  boosting types\n",
    "\n",
    "- Weight based\n",
    "- Residual based\n",
    "\n",
    "### Weight based boosting\n",
    "\n",
    "Model calculates weights (by adding absolute error to prediction)\n",
    "and new model will use weights to generate new training sample. For example, if you specific row weight=2, means on model sample the row will be presented twice. Generally, each sequential model, works mostly on train samples with poor performance from previous model. \n",
    "\n",
    "Example provided below:\n",
    "\n",
    "![image.png](images/wbb_1.png)\n",
    "\n",
    "![image.png](images/wbb_2.png)\n",
    "\n",
    "![image.png](images/wbb_3.png)\n",
    "\n",
    "![image.png](images/wbb_4.png)\n",
    "\n",
    "![image.png](images/wbb_5.png)\n",
    "\n",
    "Weight based boosting parameters.\n",
    "\n",
    "- Learning rate(or shrinkage or eta). Assign values through cross-validation to not overfit.\n",
    "- Number of estimators. (if increase number of estimators, decrease learning rate). Assign values through cross-validation to not overfit).\n",
    "- Input mode - can be anaything that accepts weights\n",
    "- Sub boosting type:\n",
    "    - AdaBoost - Good implementation in sklearn(python)\n",
    "    \n",
    "### Residual based boosting\n",
    "\n",
    "Residual based boosting working idea: Model_1 predicts `y`, then we compute error(not absolute error, real error, because direction of error matters). Then Model_2 predicts Model_1 Error. So, final prediction Model_1 `y_pred` + Model_2 `model_1_error_pred` * learning rate(eta). We multiply by learning rate becasue we don't want to overrely on one model. Generally `predictionN = pred0 + pred1*eta +...+predN*eta`.\n",
    "\n",
    "![image.png](images/rbb_1.png)\n",
    "\n",
    "![image.png](images/rbb_2.png)\n",
    "\n",
    "![image.png](images/rbb_3.png)\n",
    "\n",
    "![image.png](images/rbb_4.png)\n",
    "\n",
    "Residual based boosting parameters\n",
    "\n",
    "- Learning rate(or shrinkage or eta). Assign values through cross-validation to not overfit.\n",
    "- Number of estimators. (if increase number of estimators, decrease learning rate). Assign values through cross-validation to not overfit).\n",
    "- Row(sub) sampling.\n",
    "- Column(sub) sampling.\n",
    "- Input model - better be trees.\n",
    "- Sub boosting type:\n",
    "    - Fully gradient based. (explained above)\n",
    "    - Dart. Uses dropout. For example we build 10 models and droupout_rate = 0.2, then model_11 will use only 8 models (randomly drop 0.2 of total trees). Works as a form of regularization. Very efficent for classification.  \n",
    "    \n",
    "    \n",
    "Residual based favorite implementations.\n",
    "- Xgboost\n",
    "- Lightgbm \n",
    "- H20's GBM (out of the box categorical variables handling)\n",
    "- Catboost (handles categorical variables + strong initial set of parameters, meaning less tuning needed)\n",
    "- Sklearn's GBM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking\n",
    "\n",
    "\n",
    "### Defininition\n",
    "\n",
    "`Stacking` means making predictions of a number of models in a hold-out set and then using a different(Meta) model to train on these predictions \n",
    "\n",
    "\n",
    "![image.png](images/stacking_1.png)\n",
    "\n",
    "![image.png](images/stacking_2.png)\n",
    "\n",
    "![image.png](images/stacking_3.png)\n",
    "\n",
    "![image.png](images/stacking_4.png)\n",
    "\n",
    "![image.png](images/stacking_5.png)\n",
    "\n",
    "![image.png](images/stacking_6.png)\n",
    "\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "`Base learner` are models used to make train set for `Meta model`. \n",
    "\n",
    "`Meta model` is a higher level learner uses train data composed from predictions of base learner. Prefix `meta` means `after`.\n",
    "\n",
    "In the example below:\n",
    "- `Base learners`: algorithm_0, algorithm_1, algorithm_2\n",
    "- `Meta model`: algorithm_3\n",
    "![image.png](images/stacking_7.png)\n",
    "\n",
    "![image.png](images/stacking_8.png)\n",
    "\n",
    "### Good things about Stacking\n",
    "\n",
    "In the models averaging example, we concluded if age<50 then use model_1, else use model_2. But there is no way to know age without looking at target data. Meaning, if we look at the target data, then there is no point, because we want to predict and not look at it. As a result, model averaging example is fully theoretical and not implementabel.\n",
    "\n",
    "Stacking uses base learners stregths and tries to make combined model taking only stregths of each model, and was able to reach quite the same R^2 score. This is way stacking is very powerful, always use it.\n",
    "![image.png](images/stacking_9.png)\n",
    "\n",
    "\n",
    "### Things to be mindful of\n",
    "\n",
    "- With time sensitive data - respect time. Meaning validation set should in future time than train set, and test set should be in future time than validation set.\n",
    "\n",
    "- Diversity as important as performance, because each model has its stregth and finds hidden relationship. More diferse base models you use, more hidden relationship you can capture, and reach higher scores on test set. \n",
    "\n",
    "- Diversity mat come from:\n",
    "    - Different algorithms. Use different class of algorithms, to bring diversity. For exaple, use base_learner_1 = linaer model to capture linear dependency, and use base_learner_2 = tree_model to capture non-linear relationships. \n",
    "    - Different input features. Try to use different subset of columns and rows. Try to use different encodings for categorical variables. For example, for base_leaerner_1 use one-hot-encoding, and for base_learner_2 use label encoding. \n",
    "\n",
    "- Performance plateauing after N models\n",
    "\n",
    "- Meta model is normallu modest. Use complex models for base learners, becasue they should find different patterns. Use modest(not-so complex) model for meta model, because all patterns found by base learner, meta models just learns how to combine them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StackNet\n",
    "\n",
    "### Definition\n",
    "\n",
    "A scalable(because we can train base learners in parralel) meta modeling technology that utilizes stacking to combine multiple models in a neural network architecture of multiple levels. \n",
    "\n",
    "Example illustrates how StackNet works:\n",
    "\n",
    "![image.png](images/stacknet_1.png)\n",
    "\n",
    "![image.png](images/stacknet_2.png)\n",
    "\n",
    "![image.png](images/stacknet_3.png)\n",
    "\n",
    "![image.png](images/stacknet_4.png)\n",
    "\n",
    "![image.png](images/stacknet_5.png)\n",
    "\n",
    "![image.png](images/stacknet_6.png)\n",
    "\n",
    "![image.png](images/stacknet_7.png)\n",
    "\n",
    "We do not use hold out set, because we needed to re-split again on each layer. So, we use K-Fold method, and for each fold we validate it, with remaining fold, and added to new pred column on respective fold row. After we finish validating all folds, we can use pred column to train it in meta model on given layer. \n",
    "![image.png](images/stacknet_8.png)\n",
    "\n",
    "\n",
    "![image.png](images/stacknet_9.png)\n",
    "\n",
    "![image.png](images/stacknet_10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips\n",
    "\n",
    "\n",
    "### 1st level tips:\n",
    "\n",
    "- Diversity based on algorithms:\n",
    "    - 2-3 gradient boosted trees. (lightgbm, xgboost, H20, catboost). One deep depth, one medium depth, one shallow depth. Then tune the parameters around them, and make them have similar performance as possible.\n",
    "    - 2-3 Neural nets (keras, pythorch). One deep NN(3 hidden layers). One middel NN(2 hidden layers). One shallow (1 hidden layer). Why because try to deversify and get a new information.\n",
    "    - 1-2 ExtraTrees/RandomForest(sklearn).\n",
    "    - 1-2 linear models as in logistic/ridge regression, linear svm(sklearn).\n",
    "    - 1-2 knn models (sklearn). KNN models usually add quite nice value in metamodeling context. But when run knn individually performance is not good as others(xgboost).\n",
    "    - 1 factorization machine (libfm). Factorizes all pairwise interactions.\n",
    "    - 1 svm with non linear kernel(RBF) if size/memory allows(sklearn).\n",
    "    \n",
    "- Diversity based on input data:\n",
    "    -  Categorical Features: One hot encododing, label encoding, target encoding. \n",
    "    - Numerical Features: outliers(take care of outliers and don't take care of outliers), binning (for example from x to z from z to all), derivatives (way of smoothen your variables), percentiles, scaling.\n",
    "    - Interactions: col1*/+-col2, groupby(for example average of one group), unsupervised(k-means, SVD, PCA).\n",
    "    \n",
    "    \n",
    "### Sebsequent level tips\n",
    "\n",
    "- Simpler(or shallower) Algorithms:\n",
    "    - Gradient boosted trees with small depth (like 2 or 3).\n",
    "    - Linear models with high regularization.\n",
    "    - ExtraTrees(don't make them too big)\n",
    "    - Shallow networks (as in 1 hidden layer)\n",
    "    - knn with BrayCurtisDistance\n",
    "    \n",
    "- Feature engineering:\n",
    "    -  pairwise difference between meta models predictions.(When you create a difference, you essentially force the model to focus on what each new model brings).\n",
    "    - row-wise statistics like averages or stds\n",
    "    - Standard feature selection techniques. (Don't know them, on on my mind is Feature importance on RandomForest/Xgboost)\n",
    "\n",
    "- Rule of thumb. For every 7.5 models in previous level we add 1 in meta. (For example if we have 7 models then we have 1 meta-model, if we have 15 models then we have 2 meta models, and so on)\n",
    "\n",
    "- Be mindul of target leakage. (How we can control this, by selecting right 'k' on k-fold cross validation. So when we select a very high 'k' value, this means that each model would use more training data, when it makes, and therefor might not generalize well. At the same time, it will exhaust all information about training data. There is no easy way to spot a mistake here, normally you have a test data set, and if you see in your cross-validation that you have a next improvement, that in your test data you don't see it. Then you need to go back and try to reduce the number of K-folds.\n",
    "\n",
    "\n",
    "\n",
    "### Software for Stacking\n",
    "\n",
    "- StackNet (https://github.com/kaz-Anova/StackNet). It has parameters section, which is usefull even outside of StackNet. For example which parameters are important for Xgboost. (https://github.com/kaz-Anova/StackNet/blob/master/parameters/PARAMETERS.MD) \n",
    "- Stacked ensembles from H20 \n",
    "- Xcessiv (https://github.com/reiinakano/xcessiv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation shema for 2-nd level models:\n",
    "\n",
    "There are a number of ways to validate second level models (meta-models). In this reading material you will find a description for the most popular ones. If not specified, we assume that the data does not have a time component. We also assume we already validated and fixed hyperparameters for the first level models (models).\n",
    "\n",
    "a) Simple holdout scheme\n",
    "\n",
    "    1. Split train data into three parts: partA and partB and partC.\n",
    "    2. Fit N diverse models on partA, predict for partB, partC, test_data getting meta-features partB_meta, partC_meta and test_meta respectively.\n",
    "    3. Fit a metamodel to a partB_meta while validating its hyperparameters on partC_meta.\n",
    "    4. When the metamodel is validated, fit it to [partB_meta, partC_meta] and predict for test_meta.\n",
    "\n",
    "b) Meta holdout scheme with OOF meta-features\n",
    "\n",
    "    1. Split train data into K folds. Iterate though each fold: retrain N diverse models on all folds except current fold, predict for the current fold. After this step for each object in train_data we will have N meta-features (also known as out-of-fold predictions, OOF). Let's call them train_meta.\n",
    "    2. Fit models to whole train data and predict for test data. Let's call these features test_meta.\n",
    "    3. Split train_meta into two parts: train_metaA and train_metaB. Fit a meta-model to train_metaA while validating its hyperparameters on train_metaB.\n",
    "    4. When the meta-model is validated, fit it to train_meta and predict for test_meta.\n",
    "\n",
    "c) Meta KFold scheme with OOF meta-features\n",
    "\n",
    "    1. Obtain OOF predictions train_meta and test metafeatures test_meta using b.1 and b.2.\n",
    "    2. Use KFold scheme on train_meta to validate hyperparameters for meta-model. A common practice to fix seed for this KFold to be the same as seed for KFold used to get OOF predictions.\n",
    "    3. When the meta-model is validated, fit it to train_meta and predict for test_meta.\n",
    "\n",
    "d) Holdout scheme with OOF meta-features\n",
    "\n",
    "    1. Split train data into two parts: partA and partB.\n",
    "    2. Split partA into K folds. Iterate though each fold: retrain N diverse models on all folds except current fold, predict for the current fold. After this step for each object in partA we will have N meta-features (also known as out-of-fold predictions, OOF). Let's call them partA_meta.\n",
    "    3. Fit models to whole partA and predict for partB and test_data, getting partB_meta and test_meta respectively.\n",
    "    4. Fit a meta-model to a partA_meta, using partB_meta to validate its hyperparameters.\n",
    "    5. When the meta-model is validated basically do 2. and 3. without dividing train_data into parts and then train a meta-model. That is, first get out-of-fold predictions train_meta for the train_data using models. Then train models on train_data, predict for test_data, getting test_meta. Train meta-model on the train_meta and predict for test_meta.\n",
    "\n",
    "e) KFold scheme with OOF meta-features\n",
    "\n",
    "    1. To validate the model we basically do d.1 -- d.4 but we divide train data into parts partA and partB M times using KFold strategy with M folds.\n",
    "    2. When the meta-model is validated do d.5.\n",
    "\n",
    "##### Validation in presence of time component\n",
    "\n",
    "f) KFold scheme in time series\n",
    "\n",
    "In time-series task we usually have a fixed period of time we are asked to predict. Like day, week, month or arbitrary period with duration of T.\n",
    "\n",
    "    1. Split the train data into chunks of duration T. Select first M chunks.\n",
    "    2. Fit N diverse models on those M chunks and predict for the chunk M+1. Then fit those models on first M+1 chunks and predict for chunk M+2 and so on, until you hit the end. After that use all train data to fit models and get predictions for test. Now we will have meta-features for the chunks starting from number M+1 as well as meta-features for the test.\n",
    "    3. Now we can use meta-features from first K chunks [M+1,M+2,..,M+K] to fit level 2 models and validate them on chunk M+K+1. Essentially we are back to step 1. with the lesser amount of chunks and meta-features instead of features.\n",
    "\n",
    "g) KFold scheme in time series with limited amount of data\n",
    "\n",
    "We may often encounter a situation, where scheme f) is not applicable, especially with limited amount of data. For example, when we have only years 2014, 2015, 2016 in train and we need to predict for a whole year 2017 in test. In such cases scheme c) could be of help, but with one constraint: KFold split should be done with the respect to the time component. For example, in case of data with several years we would treat each year as a fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "Chek-out implementation here on Week 4, Assignment. https://github.com/Brandon-HY-Lin/coursera_How-to-Win-a-Data-Science-Competition-Learn-from-Top-Kagglers\n",
    "\n",
    "\n",
    "\n",
    "# Reading\n",
    "\n",
    "Kaggle ensembling guide at MLWave.com (overview of approaches): https://mlwave.com/kaggle-ensembling-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
