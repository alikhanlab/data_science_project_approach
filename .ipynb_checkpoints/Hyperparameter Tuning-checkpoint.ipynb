{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning \n",
    "\n",
    "\n",
    "### Workflow:\n",
    "\n",
    "1. Select the most influential parameters.\n",
    "    - There are tons of parameters and we can't tune all of them.(It will take forever to tune).\n",
    "\n",
    "2. Understand how exactly they influence the training.\n",
    "\n",
    "3. Tune them:\n",
    "    - Manually (change and examine). (Most people use)\n",
    "    - Automatically. (hyperopt, sckit-optimize for example)\n",
    "    \n",
    "    \n",
    "### Method\n",
    "\n",
    "We use color coding method. Color - coding legend (for hyperparameter optmization). When we optmize parameters it can be resulted in 3 behaviors. \n",
    "\n",
    "1. Undefitting(bad). [Red color]\n",
    "2. Good fit and generalization [Black color]\n",
    "3. Overfitting(bad).[Green color]\n",
    "\n",
    "So we need to devide all parameters we would like to tune in 2 groups(because if good fit no need to tune black color). \n",
    "\n",
    "1. [Red color] parameter. Constraints the model. Simplifies model.\n",
    "    - Increase it to reduce overfitting.\n",
    "    - Decrease to allow model fit easier.\n",
    "\n",
    "2. [Green color] parameter.\n",
    "    - Increase it if model underfits.\n",
    "    - Decrease it if overfits.\n",
    "\n",
    "\n",
    "### Hyperparameter optimization for tree based models.\n",
    "\n",
    "XGBoost, LightGBM, CatBoost are gold industry standards. (2018)\n",
    "\n",
    "XGboost, LightGBM. - how it works. Model builds decision trees one after another gradually optimizing a given objecctive. So there are many parameters controls tree building process. So we need to optimize it.\n",
    "\n",
    "![image.png](images/hyperparameter_tuning.png)\n",
    "\n",
    "\n",
    "### Hyperparameter tuning for RandomForest\n",
    "\n",
    "- `n_estimator` [Black color] (the higher is better). But it pleteau at some point. n_estimators = 40 is pretty good.\n",
    "\n",
    "- `max_depth` [Green color].\n",
    "- `max_features` [Green color]\n",
    "- `min_samples_leaf` [Red color]\n",
    "- `critertion`. Gini mostly better, but sometimes Enthropy can be better.\n",
    "- `n_jobs=-1`. To use all cores.\n",
    "\n",
    "### Hyperparameter tuning for Neural Nets.\n",
    "\n",
    "Use: \n",
    "- Pytorch\n",
    "- Keras\n",
    "- Tensorflow\n",
    "\n",
    "We only use dense NN, fully connected layers.\n",
    "\n",
    "- `Number of neurons per layer`. [Green color]. Start with 64 units.\n",
    "\n",
    "\n",
    "- `Number of layers`. [Green color]. Start with 2 layers.\n",
    "\n",
    "\n",
    "- `Optimizers`:\n",
    "    - SGD + momentum [Red color].\n",
    "    - Adam, Adadelta, Adagrad [Green color]. Fast, but overfits. Use problems other than classification and regression. \n",
    "    \n",
    "    \n",
    "- `Batch size`. [Green color]. Satrt with 32-64. If overfits decrease, if underfits increase.\n",
    "\n",
    "\n",
    "- `Learning rate`. Start with huge learning rate 0.1, then go lower where network converges. \n",
    "\n",
    "\n",
    "- If you increase batch size by factor of alpha, you can also increase learning rate by the same factor. \n",
    "\n",
    "- `Regularization`. [Red Color]\n",
    "    - L2/L1 for weights \n",
    "    - Dropout/Dropconnect\n",
    "    \n",
    "        Technique:\n",
    "        \n",
    "        Whenever you see NN overfitting, first try dropout layer. Override dropout probability and a place where you insert the dropout layer. Usually add to close to end of network, but it's okay to add some dropout to every layer. Dropout helps network to find features that really matters. Do not add at very beginning.\n",
    "        \n",
    "    - Static dropconnect\n",
    "    ![image.png](images/nn_dropconnect.png)\n",
    "    \n",
    "    Then regularize, at random drop 99% of connections between input layer and hidden layer 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips\n",
    "\n",
    "1. Don't spend too much time on tuning. (Appropriate features and hacks)\n",
    "2. Be patient. Takes thousands of rounds GBDT or NN to fit.\n",
    "3. Average everything.\n",
    "    - Over random seeds\n",
    "    - Or over small deriviations. Optimal parameters for example average max_depth=4, max_depth=5, max_depth=6 for optimal max_depth = 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
